# 产品设计原则

> 本文档将 Univers 的[核心哲学](../PHILOSOPHY.md)转化为具体的产品设计原则和实践指南。

## 核心理念

Univers 的产品设计围绕一个核心问题：

**如何让人类和 AI 高效协作，创造单独都无法实现的价值？**

我们的答案：
- **人类定义目标和价值**（在 workbench）
- **AI 执行和优化**（在 operation）
- **产品是协作的桥梁**（让意图清晰表达，让执行透明可控）

## 设计原则

### 1. 目标优先于操作

**理念**：用户应该表达"想要什么"，而非"怎么做"

❌ **传统方式**：
```
用户操作：
1. 打开温度设置面板
2. 选择 3 楼区域
3. 调整温度滑块到 22°C
4. 选择"节能模式"
5. 设置时间段
6. 点击保存
```

✅ **Univers 方式**：
```
用户说：
"3 楼在保证舒适的前提下，尽量省电"

系统理解：
- 目标：省电
- 约束：舒适度优先
- 范围：3 楼
- AI 自动执行和持续优化
```

**产品实现**：
- 提供自然语言输入或结构化的目标设定表单
- 让用户定义"什么重要"（优先级、权重）
- AI 负责找到实现路径

### 2. 透明而非黑盒

**理念**：用户应该理解 AI 在做什么，为什么这样做

❌ **错误做法**：
```
"AI 自动优化中..."
[30 分钟后]
"优化完成，已节省 15% 电费"
```
用户疑问：它做了什么？安全吗？我信任吗？

✅ **正确做法**：
```
"AI 正在优化 3 楼能效..."

实时展示：
✓ 调整了 3 个空调温度设定（24°C → 23°C）
✓ 优化了新风系统运行时段（延迟 30 分钟启动）
⏸ 等待人流数据确认是否调整 4 楼

预计效果：
- 节省电费：15%
- 舒适度：保持 85 分以上
- 风险：低
```

**产品实现**：
- 实时展示 AI 的决策过程
- 解释每个决策的原因（"因为过去 1 小时人流下降 40%"）
- 可视化影响和预期效果
- 允许用户随时查看详细日志

### 3. 默认信任，保留控制

**理念**：让 AI 自动运行，但人类随时可以干预

**渐进式自动化**：
```
Level 0: 手动操作
  ↓
Level 1: AI 提供建议 → 人类决定
  ↓
Level 2: AI 自动执行小范围 → 人类监督
  ↓
Level 3: AI 自动执行大范围 → 异常时人类干预
  ↓
Level 4: AI 完全自主 → 人类定期审查
```

用户可以选择信任程度。

**产品实现**：
- 提供"自主程度"设置（保守 / 平衡 / 激进）
- 任何时候都有"暂停 AI"按钮
- 重要操作需要人类确认
- 建立信任后，逐步放权

**示例界面**：
```
┌─────────────────────────────────────┐
│ AI 自主程度设置                      │
├─────────────────────────────────────┤
│ 温度调节：                           │
│   ○ 保守：±1°C，需审批               │
│   ● 平衡：±2°C，自动执行              │
│   ○ 激进：±3°C，自动执行              │
│                                      │
│ 设备控制：                           │
│   ● 开关模式：自动                    │
│   ○ 设备重启：需审批                  │
│   ○ 维护模式：禁止                    │
└─────────────────────────────────────┘
```

### 4. 上下文而非数据

**理念**：展示有意义的信息，而非原始数据

❌ **错误做法**：
```
3F-HVAC-01: 24.5°C, 55% RH, 1200 CFM
3F-HVAC-02: 23.8°C, 58% RH, 1100 CFM
3F-HVAC-03: 25.2°C, 52% RH, 1250 CFM
```
用户困惑：这些数字是好是坏？

✅ **正确做法**：
```
3 楼环境：良好 ✓
- 温度：舒适范围内
- 湿度：正常
- 空气质量：优

⚠️ 3F-HVAC-03 温度偏高
  原因：下午日晒强
  AI 已自动调整，预计 15 分钟后恢复正常
```

**产品实现**：
- 用语义化的状态替代数字（良好/警告/异常）
- 提供趋势而非瞬时值
- 突出需要关注的异常
- 用户需要时才展开详细数据

### 5. 反馈而非单向

**理念**：系统从人类反馈中学习，持续改进

**反馈循环**：
```
AI 执行决策
    ↓
用户感受结果（舒适/不舒适）
    ↓
用户反馈（👍/👎 或调整参数）
    ↓
系统学习偏好
    ↓
AI 下次做得更好
```

**产品实现**：
- 轻量级反馈机制（一键点赞/点踩）
- 询问"为什么不满意？"（太冷/太热/太吵）
- 记录用户手动调整，作为隐式反馈
- 定期展示"AI 从你的反馈中学到了什么"

**示例**：
```
┌─────────────────────────────────────┐
│ AI 刚才降低了 3 楼温度               │
│ 你觉得这个决策怎么样？               │
│                                      │
│   👍 很好    👎 不好    → 详细反馈   │
└─────────────────────────────────────┘

[用户点击 👎]

┌─────────────────────────────────────┐
│ 请问是什么问题？                     │
│   ☐ 太冷了                          │
│   ☐ 反应太慢                        │
│   ☐ 不该在这个时间调整               │
│   ☐ 其他：_________                 │
│                                      │
│   [提交反馈]                         │
└─────────────────────────────────────┘
```

### 6. 授权而非依赖

**理念**：让用户变得更强大，而不是更依赖

❌ **错误做法**：
- 隐藏所有细节
- 用户完全依赖 AI
- 出问题时用户无能为力

✅ **正确做法**：
- 提供"学习模式"，解释 AI 的逻辑
- 用户可以选择手动操作
- 提供"如果 AI 出问题怎么办"的应急方案
- 让用户理解系统，建立掌控感

**产品实现**：
- "AI 解释模式"：详细说明每个决策
- "手动覆盖"：用户可以随时接管
- "应急手册"：AI 故障时的操作指南

## 用户角色与界面设计

基于[核心哲学](../PHILOSOPHY.md)中定义的人类角色，设计不同的界面：

### 角色 1：目标设定者
**界面**：目标设定面板

```
┌─────────────────────────────────────┐
│ 优化目标设定                         │
├─────────────────────────────────────┤
│ 主要目标：                           │
│   ● 节能优先                         │
│   ○ 舒适优先                         │
│   ○ 成本优先                         │
│                                      │
│ 约束条件：                           │
│   ☑ 温度保持在 20-26°C               │
│   ☑ 舒适度评分 > 80                  │
│   ☐ 工作时间优先响应                 │
│                                      │
│ 应用范围：                           │
│   [选择楼层/区域]                    │
│                                      │
│   [保存并启动 AI]                    │
└─────────────────────────────────────┘
```

### 角色 2：策略审批者
**界面**：审批中心

```
┌─────────────────────────────────────┐
│ 待审批的 AI 建议                     │
├─────────────────────────────────────┤
│ ⚠️ AI 建议：调整全楼供冷策略         │
│                                      │
│ 预期效果：节省 20% 电费              │
│ 影响范围：全楼                       │
│ 风险评估：中                         │
│                                      │
│ 详细方案：                           │
│ - 提前 1 小时预冷                    │
│ - 高峰期降低冷量 15%                 │
│ - 夜间切换到低功耗模式               │
│                                      │
│   [批准]  [拒绝]  [调整后批准]       │
└─────────────────────────────────────┘
```

### 角色 3：监督者
**界面**：实时监控大屏

```
┌─────────────────────────────────────┐
│ AI 运行状态                          │
├─────────────────────────────────────┤
│ 运行中：hvac-operation               │
│ 自主级别：平衡                       │
│ 今日决策：127 次                     │
│ 人工干预：2 次                       │
│                                      │
│ 最近行动：                           │
│ ✓ 10:23 优化 3F 温度                │
│ ✓ 10:15 调整新风系统                │
│ ⏸ 10:05 暂停 4F 优化（等待确认）    │
│                                      │
│ [查看详细日志] [暂停 AI] [设置]     │
└─────────────────────────────────────┘
```

### 角色 4：异常处理者
**界面**：告警和干预面板

```
┌─────────────────────────────────────┐
│ ⚠️ 需要人工介入                      │
├─────────────────────────────────────┤
│ 3F-HVAC-02 温度异常                  │
│                                      │
│ AI 分析：                            │
│ - 传感器读数突然下降 5°C             │
│ - 可能原因：传感器故障 / 设备异常    │
│ - 置信度：低（超出训练范围）         │
│                                      │
│ 建议行动：                           │
│ 1. 派人现场检查                      │
│ 2. 切换到备用传感器                  │
│ 3. 暂停该区域自动控制                │
│                                      │
│   [执行建议] [手动处理] [忽略]       │
└─────────────────────────────────────┘
```

## 设计模式库

### 模式 1：意图表达
**场景**：用户想要 AI 做某事，但不想指定细节

**设计**：
- 提供高层目标选择（节能/舒适/成本）
- 提供约束条件设置（温度范围、时间段）
- AI 自动生成执行计划
- 用户确认后执行

### 模式 2：AI 解释
**场景**：用户想知道"AI 为什么这样做"

**设计**：
- 每个 AI 决策都有"为什么"按钮
- 点击展开详细解释
- 使用简单语言，避免技术术语
- 提供相关数据和依据

### 模式 3：渐进式信任
**场景**：新用户刚开始使用，不确定是否信任 AI

**设计**：
- 初始默认"保守模式"
- AI 每次执行前询问
- 积累几次成功后，提示"要不要试试自动模式"
- 用户可随时回退

### 模式 4：反馈收集
**场景**：系统想了解用户满意度

**设计**：
- 非侵入式：不影响主流程
- 轻量级：一键反馈
- 上下文化：在相关时机询问（AI 执行后）
- 可选详细反馈

### 模式 5：应急接管
**场景**：AI 出问题或用户不满意，需要立即接管

**设计**：
- 全局"暂停 AI"按钮，随处可见
- 暂停后自动切换到手动模式
- 显示当前状态和建议操作
- 提供"恢复 AI"选项

## 衡量标准

好的产品设计应该达到以下指标：

### 1. 用户掌控感
- **测量**：用户问卷"我觉得对系统有掌控"（1-5 分）
- **目标**：> 4.0 分

### 2. AI 信任度
- **测量**：愿意使用"自动模式"的用户占比
- **目标**：> 60%

### 3. 干预率
- **测量**：AI 决策中，人工干预的比例
- **目标**：< 5%（太高说明 AI 不够好，太低说明用户不敢干预）

### 4. 反馈率
- **测量**：用户主动反馈的比例
- **目标**：> 20%

### 5. 学习曲线
- **测量**：新用户多久能完成第一次目标设定
- **目标**：< 10 分钟

## 反模式（避免）

### ❌ 反模式 1：AI 全知全能
**错误**：宣称 AI 可以解决一切问题

**后果**：
- 过度承诺，实际做不到
- 用户期望过高，失望后难以挽回

**正确做法**：
- 清晰告知 AI 的能力边界
- 说明什么情况需要人类介入

### ❌ 反模式 2：隐藏复杂度
**错误**：为了"简单"，隐藏所有细节

**后果**：
- 用户不理解系统
- 缺乏掌控感
- 出问题时无能为力

**正确做法**：
- 默认简单，但提供"高级选项"
- 用户想了解时，可以深入

### ❌ 反模式 3：强制自动化
**错误**：不给用户手动选项

**后果**：
- 用户感到被控制
- 特殊情况无法处理

**正确做法**：
- 始终保留手动模式
- 尊重用户选择

### ❌ 反模式 4：忽视反馈
**错误**：不关注用户反馈，AI 一成不变

**后果**：
- 系统不适应用户偏好
- 用户感觉"AI 不懂我"

**正确做法**：
- 主动收集反馈
- 展示"AI 学到了什么"
- 个性化调整

## 相关文档

- [核心哲学](../PHILOSOPHY.md) - 理解产品设计的理念基础
- [三层架构决策](../decisions/001-三层架构设计.md) - 理解系统如何支撑产品设计
- [系统架构](../tech/architecture.md) - 技术实现细节

---

**最后更新**：2025-11-02
**维护者**：产品团队

> 这些原则不是规则，而是指南。具体设计时，要根据实际场景灵活应用。
